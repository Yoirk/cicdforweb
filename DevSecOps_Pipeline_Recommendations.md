# DevSecOps Pipeline Recommendations
**Assessment Date:** January 16, 2026  
**Target:** CI/CD Pipeline for wanderingtomes.site

---

## Executive Summary

This pipeline demonstrates strong security foundations with SAST, DAST, SBOM generation, and container hardening. Key improvement areas: **runtime security monitoring**, **secret rotation**, and **disaster recovery capabilities**.

---

## High Priority (Critical Security & Reliability)

### 1. Implement Strict Image Signature Verification ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Prevents supply chain attacks  
**Effort:** Low (2-4 hours)

**Current Issue:** Verification uses loose regex that could be bypassed.

**Solution:**
```yaml
# In deploy job, replace regex-based verify with:
- name: Verify Image Signatures (Strict)
  run: |
    # Verify with specific commit SHA (immutable reference)
    cosign verify \
      --certificate-identity "https://github.com/${REPO_LC}/.github/workflows/deploy.yml@refs/heads/main" \
      --certificate-oidc-issuer "https://token.actions.githubusercontent.com" \
      ghcr.io/${REPO_LC}:${{ github.sha }}
```

---

### 2. Add SBOM Attestation & SLSA Provenance ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Supply chain transparency, compliance requirement for 2026+  
**Effort:** Medium (4-8 hours)

**Solution:**
```yaml
- name: Generate SLSA Provenance
  uses: slsa-framework/slsa-github-generator/.github/workflows/generator_container_slsa3.yml@v1.9.0
  with:
    image: ghcr.io/${{ env.REPO_LC }}
    digest: ${{ steps.build.outputs.digest }}

- name: Attach SBOM Attestation
  run: |
    syft ghcr.io/${REPO_LC}:${{ github.sha }} -o spdx-json > sbom.spdx.json
    cosign attest --predicate sbom.spdx.json \
      --type spdx ghcr.io/${REPO_LC}:${{ github.sha }}
```

---

### 3. Harden Secret Management ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Eliminates long-lived credential risks  
**Effort:** High (1-2 days)

**Current Issue:** SSH keys stored as secrets have unlimited validity.

**Solution:**
```yaml
# Option A: Tailscale SSH (Recommended)
- name: Setup Tailscale
  uses: tailscale/github-action@v2
  with:
    oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
    oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
    tags: tag:ci

- name: Deploy via Tailscale SSH
  run: |
    tailscale ssh ${{ secrets.VPS_HOST }} "cd /opt/app && docker-compose pull && docker-compose up -d"

# Option B: GitHub OIDC + Self-Hosted Runner
# Configure runner on VPS with OIDC trust
```

---

### 4. Add Container Runtime Security ‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Detects breaches and anomalies in production  
**Effort:** Medium (4-8 hours)

**Solution:**
```yaml
# Add to docker-compose.yml
services:
  falco:
    image: falcosecurity/falco:latest
    privileged: true
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc:/host/etc:ro
    environment:
      - FALCO_GRPC_ENABLED=true
      - FALCO_GRPC_BIND_ADDRESS=0.0.0.0:5060
    command:
      - /usr/bin/falco
      - --cri /var/run/docker.sock
      - -K /var/run/secrets/kubernetes.io/serviceaccount/token
      - -k https://kubernetes.default
      - -pk

  # Alert webhook for Falco events
  falco-webhook:
    image: falcosecurity/falcosidekick:latest
    environment:
      - DISCORD_WEBHOOKURL=${{ secrets.DISCORD_WEBHOOK }}
```

---

## Medium Priority (Enhanced Detection & Response)

### 5. Implement Dependency Auto-Merge ‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Reduces patch lag, minimizes manual overhead  
**Effort:** Low (1-2 hours)

**Solution:**
```yaml
# Add to .github/dependabot.yml
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/backend"
    schedule:
      interval: "daily"
    labels:
      - "dependencies"
      - "security"
    reviewers:
      - "yoirk"
    # Auto-merge configuration
    open-pull-requests-limit: 10

# Create .github/workflows/auto-merge-dependabot.yml
name: Auto-merge Dependabot PRs
on:
  pull_request:
    branches: [main]
jobs:
  auto-merge:
    if: github.actor == 'dependabot[bot]'
    runs-on: ubuntu-latest
    steps:
      - name: Auto-merge patch updates
        run: |
          if [[ "${{ github.event.pull_request.title }}" =~ bump.*from.*to.*[0-9]+\.[0-9]+\.[0-9]+ ]]; then
            gh pr merge --auto --squash "${{ github.event.pull_request.number }}"
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

---

### 6. Add License Compliance Scanning ‚≠ê‚≠ê‚≠ê
**Impact:** Prevents GPL/copyleft violations  
**Effort:** Low (2-3 hours)

**Solution:**
```yaml
- name: License Scan
  uses: fossas/fossa-action@v1
  with:
    api-key: ${{ secrets.FOSSA_API_KEY }}
    
# Or open-source alternative:
- name: License Check (Scancode)
  run: |
    pip install scancode-toolkit
    scancode --license --copyright --package --json-pp license-report.json backend/
    # Fail if GPL/AGPL detected
    if grep -q '"AGPL\|GPL"' license-report.json; then
      echo "‚ùå Copyleft license detected!"
      exit 1
    fi
```

---

### 7. Implement Chaos Engineering Tests ‚≠ê‚≠ê‚≠ê
**Impact:** Validates resilience under failure  
**Effort:** Medium (4-6 hours)

**Solution:**
```yaml
- name: Chaos Test - Network Latency
  run: |
    # Inject 3s latency to backend
    docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
      gaiaadm/pumba netem --duration 1m delay \
      --time 3000 python_api
    
    # Verify frontend still responds within SLA
    response_time=$(curl -o /dev/null -s -w '%{time_total}' http://localhost/)
    if (( $(echo "$response_time > 5.0" | bc -l) )); then
      echo "‚ùå SLA violation: ${response_time}s"
      exit 1
    fi

- name: Chaos Test - Container Kill
  run: |
    docker kill python_api
    sleep 5
    # Verify docker-compose restart works
    curl --retry 3 --retry-delay 5 http://localhost/api/thoughts/random
```

---

### 8. Make API Contract Testing Blocking ‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Prevents API regressions from reaching production  
**Effort:** Low (1 hour)

**Current Issue:** Schemathesis tests run but don't block deployment on failure.

**Solution:**
```yaml
- name: API Contract Testing (Blocking)
  run: |
    st run http://localhost:8000/openapi.json \
      --checks all \
      --hypothesis-max-examples 100 \
      --hypothesis-deadline 5000 \
      --exitfirst \
      --report \
      --junit-xml=schemathesis-report.xml
  
- name: Upload Test Results
  if: always()
  uses: actions/upload-artifact@v3
  with:
    name: api-contract-test-results
    path: schemathesis-report.xml
```

---

## Medium-Low Priority (Operational Excellence)

### 9. Centralize Logging ‚≠ê‚≠ê‚≠ê
**Impact:** Faster incident response, better observability  
**Effort:** High (1-2 days)

**Solution:**
```yaml
# Add to docker-compose.yml
services:
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.0
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./promtail-config.yml:/etc/promtail/config.yml
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.2.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${{ secrets.GRAFANA_PASSWORD }}
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped

volumes:
  loki-data:
  grafana-data:
```

---

### 10. Add Performance Regression Testing ‚≠ê‚≠ê‚≠ê
**Impact:** Prevents performance degradation  
**Effort:** Medium (3-5 hours)

**Solution:**
```yaml
- name: Load Test (k6)
  run: |
    docker run --rm -i grafana/k6 run --out json=results.json - <<EOF
    import http from 'k6/http';
    import { check, sleep } from 'k6';
    
    export let options = {
      stages: [
        { duration: '30s', target: 20 },
        { duration: '1m', target: 50 },
        { duration: '20s', target: 0 },
      ],
      thresholds: {
        http_req_duration: ['p(95)<500'], // 95% requests < 500ms
        http_req_failed: ['rate<0.01'],   // Error rate < 1%
      },
    };
    
    export default function() {
      let res = http.get('http://host.docker.internal/');
      check(res, { 'status is 200': (r) => r.status === 200 });
      sleep(1);
    }
    EOF
    
- name: Compare Performance
  run: |
    # Store baseline in S3/GitHub artifacts
    # Compare current results against baseline
    # Fail if p95 latency increased by >20%
```

---

### 11. Implement Automated Backups ‚≠ê‚≠ê‚≠ê‚≠ê
**Impact:** Business continuity, disaster recovery  
**Effort:** Medium (4-6 hours)

**Current Issue:** SQLite database has no backup strategy.

**Solution:**
```yaml
- name: Backup Database
  run: |
    ssh ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }} \
      "docker exec python_api sqlite3 /data/app.db '.backup /tmp/backup-$(date +%Y%m%d-%H%M%S).db'"
    
    # Transfer backup
    scp ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }}:/tmp/backup-*.db ./
    
    # Upload to cloud storage
    aws s3 cp backup-*.db s3://wanderingtomes-backups/ \
      --storage-class GLACIER \
      --server-side-encryption AES256

# Schedule daily backups via cron on VPS:
# 0 2 * * * docker exec python_api sqlite3 /data/app.db '.backup /backups/app-$(date +\%Y\%m\%d).db'
```

---

### 12. Add Rollback Automation ‚≠ê‚≠ê‚≠ê
**Impact:** Faster incident recovery  
**Effort:** High (1-2 days)

**Solution:**
```yaml
- name: Deploy with Health Check
  id: deploy
  run: |
    ssh ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }} << 'EOF'
      cd /opt/app
      
      # Store previous image
      PREVIOUS_IMAGE=$(docker inspect python_api --format='{{.Image}}')
      echo "previous_image=$PREVIOUS_IMAGE" >> $GITHUB_OUTPUT
      
      # Deploy new version
      docker-compose pull
      docker-compose up -d
      
      # Wait for health check
      sleep 10
      
      # Check health endpoint
      for i in {1..30}; do
        if curl -f http://localhost/api/thoughts/random; then
          echo "‚úÖ Health check passed"
          exit 0
        fi
        sleep 2
      done
      
      echo "‚ùå Health check failed, rolling back"
      docker tag $PREVIOUS_IMAGE ghcr.io/repo:latest
      docker-compose up -d
      exit 1
    EOF
```

---

## Low Priority (Nice to Have)

### 13. Add Security Training Metrics ‚≠ê‚≠ê
**Impact:** Long-term security culture improvement  
**Effort:** Low (2-3 hours)

**Solution:**
```yaml
# .github/workflows/security-scorecard.yml
name: Security Scorecard
on:
  schedule:
    - cron: '0 0 * * 1'  # Weekly
jobs:
  scorecard:
    runs-on: ubuntu-latest
    steps:
      - name: Generate Scorecard
        run: |
          # Track metrics:
          # - Secrets exposed in commits (git-secrets)
          # - Dependencies updated within 30 days
          # - Security issues remediated
          # - Code review coverage
          
      - name: Post to Discord
        run: |
          curl -X POST ${{ secrets.DISCORD_WEBHOOK }} \
            -H 'Content-Type: application/json' \
            -d '{"content": "üèÜ Weekly Security Score: 87/100"}'
```

---

### 14. Implement Feature Flags ‚≠ê‚≠ê
**Impact:** Safer deployment control, A/B testing  
**Effort:** High (2-3 days)

**Solution:**
```python
# backend/main.py
from launchdarkly import LDClient, Config
from launchdarkly.integrations import Files

# Initialize LaunchDarkly (or open-source alternative: Unleash)
ld_client = LDClient(config=Config(sdk_key=os.environ.get("LD_SDK_KEY")))

@app.get("/thoughts/random")
def get_random_thoughts():
    user = {"key": "anonymous"}
    
    # Check feature flag
    if ld_client.variation("new-recommendation-algorithm", user, False):
        return get_thoughts_ml_based()  # New feature
    else:
        return get_thoughts_random()     # Existing feature

# Allows gradual rollout: 10% ‚Üí 50% ‚Üí 100%
```

---

### 15. Add Compliance Reporting ‚≠ê
**Impact:** Audit readiness  
**Effort:** High (2-3 days)

**Solution:**
```yaml
- name: Generate Compliance Report
  run: |
    # Collect evidence:
    # - All scan results (Trivy, Grype, Semgrep)
    # - Access logs (who deployed when)
    # - Change audit trail (Git history)
    # - Vulnerability remediation timeline
    
    python generate_compliance_report.py \
      --format pdf \
      --output compliance-$(date +%Y%m%d).pdf
    
    # Upload to secure storage
    aws s3 cp compliance-*.pdf s3://compliance-reports/ \
      --server-side-encryption aws:kms \
      --kms-key-id ${{ secrets.KMS_KEY_ID }}
```

---

## Critical Issues to Fix Immediately

### üî¥ Issue #1: Rate Limit Test is Flawed
**Location:** `.github/workflows/deploy.yml` line 228

**Problem:** Apache Bench may not respect Nginx rate limits properly due to keep-alive connections.

**Fix:**
```yaml
- name: Rate Limit Test (Corrected)
  run: |
    # Install vegeta
    wget https://github.com/tsenart/vegeta/releases/download/v12.11.1/vegeta_12.11.1_linux_amd64.tar.gz
    tar xzf vegeta_12.11.1_linux_amd64.tar.gz
    
    # Test rate limit (Nginx configured for 10r/s)
    echo "GET http://localhost/" | ./vegeta attack -duration=10s -rate=20 > results.bin
    ./vegeta report results.bin
    
    # Should see ~50% 429 errors (20r/s attempted, 10r/s allowed)
    SUCCESS_RATE=$(./vegeta report results.bin | grep 'Success' | awk '{print $3}')
    if (( $(echo "$SUCCESS_RATE > 0.60" | bc -l) )); then
      echo "‚ùå Rate limit not working! Success rate: $SUCCESS_RATE"
      exit 1
    fi
```

---

### üî¥ Issue #2: DAST Fails on Self-Signed Cert
**Location:** `.github/workflows/deploy.yml` line 206

**Problem:** ZAP may fail on TLS validation errors.

**Fix:**
```yaml
- name: DAST with OWASP ZAP
  run: |
    docker run --rm --network host \
      ghcr.io/zaproxy/zaproxy:stable \
      zap-baseline.py \
      -t https://localhost \
      -J zap-report.json \
      -r zap-report.html \
      --hook=/zap/wrk/hook.py \
      -z "-config connection.timeoutInSecs=60 \
          -config rules.cookie.ignorelist=localhost \
          -addonupdate \
          -addoninstall pscanrulesBeta"
    
    # Alternative: Use HTTP for CI testing, HTTPS for prod
```

---

### üî¥ Issue #3: Missing Resource Limits
**Location:** `docker-compose.yml`

**Problem:** Only backend has resource limits; web proxy doesn't.

**Fix:**
```yaml
services:
  web:
    image: nginx:alpine
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    # ... rest of config
```

---

### üî¥ Issue #4: SQLite Not Production-Ready
**Location:** `backend/main.py`

**Problem:** SQLite default journal mode has poor concurrency.

**Fix:**
```python
# backend/main.py
def get_db():
    conn = sqlite3.connect('app.db', check_same_thread=False)
    
    # Enable WAL mode for better concurrency
    conn.execute('PRAGMA journal_mode=WAL')
    conn.execute('PRAGMA synchronous=NORMAL')
    conn.execute('PRAGMA cache_size=-64000')  # 64MB cache
    
    return conn

# Better: Migrate to PostgreSQL
# docker-compose.yml:
# services:
#   db:
#     image: postgres:16-alpine
#     environment:
#       POSTGRES_PASSWORD: ${DB_PASSWORD}
#     volumes:
#       - pgdata:/var/lib/postgresql/data
```

---

## Implementation Roadmap

### Phase 1: Quick Wins (Week 1)
- ‚úÖ Fix rate limit test
- ‚úÖ Add resource limits to docker-compose
- ‚úÖ Enable SQLite WAL mode
- ‚úÖ Strengthen cosign verification

### Phase 2: Core Security (Week 2-3)
- üîí Add SLSA provenance
- üîí Implement runtime security (Falco)
- üîí Setup automated backups
- üîí License compliance scanning

### Phase 3: Operational Excellence (Week 4-6)
- üìä Centralized logging (Loki + Grafana)
- üìä Performance regression testing
- üìä Chaos engineering tests
- üìä Rollback automation

### Phase 4: Advanced (Month 2+)
- üöÄ Migrate to PostgreSQL
- üöÄ Implement secret rotation
- üöÄ Feature flags
- üöÄ Compliance reporting

---

## Cost-Benefit Analysis

| Recommendation | Security Impact | Operational Impact | Implementation Cost | ROI |
|---|---|---|---|---|
| Strict image verification | Critical | Low | Low | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Runtime security (Falco) | High | High | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Automated backups | Medium | Critical | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| SLSA provenance | High | Low | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Secret rotation | High | Low | High | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Centralized logging | Low | High | High | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Performance testing | Low | Medium | Medium | ‚≠ê‚≠ê‚≠ê |
| Feature flags | Low | Medium | High | ‚≠ê‚≠ê‚≠ê |
| Compliance reporting | Low | Low | High | ‚≠ê‚≠ê |

---

## Conclusion

**Current Pipeline Maturity:** 7.5/10

**Strengths:**
- ‚úÖ Comprehensive SAST/DAST coverage
- ‚úÖ SBOM generation
- ‚úÖ Container hardening
- ‚úÖ Multi-stage security scanning

**Critical Gaps:**
- ‚ùå Runtime security monitoring
- ‚ùå Disaster recovery plan
- ‚ùå Secret rotation strategy

**Recommended First Actions:**
1. Fix critical bugs in rate limit testing and resource limits (2 hours)
2. Implement strict image verification with SLSA (4 hours)
3. Setup automated database backups (6 hours)
4. Deploy Falco for runtime security (8 hours)

**Estimated Total Implementation Time:** 6-8 weeks for full roadmap

---

**Document Version:** 1.0  
**Last Updated:** January 16, 2026  
**Author:** DevSecOps Consultant  
**Contact:** Available for implementation support
